{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Heat\n",
    "\n",
    "In this notebook, we will perform an **extreme heat exposure assessment** for transportation infrastructure, specifically focusing on railways. The assessment is based on combining hazard data (e.g., occurence of extreme temperatures) with spatial information on railway infrastructure to understand how future climate could increase the impact of extreme heat on the performance of railway infrastructure.\n",
    "\n",
    "We will follow the steps outlined below to conduct the assessment:\n",
    "\n",
    "1. **Loading the necessary packages:**  \n",
    "   We will import the Python libraries required for data handling, analysis, and visualization.\n",
    "\n",
    "2. **Loading the data:**  \n",
    "   The infrastructure data (e.g., railways) and hazard data (e.g., PGA levels) will be loaded into the notebook.\n",
    "\n",
    "3. **Preparing the data:**  \n",
    "   The infrastructure and hazard data will be processed and data gaps can be filled, if required.\n",
    "\n",
    "4. **Performing the exposure assessment:**  \n",
    "   We will overlay the railway infrastructure data with data on extreme heat.\n",
    "\n",
    "5. **Visualizing the results:**  \n",
    "   Finally, we will visualize the estimated damage using graphs and maps to illustrate the impact on railway infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Necessary Packages\n",
    "To perform the assessment, we are going to make use of several [python packages](https://docs.python.org/3/tutorial/modules.html#packages).\n",
    "\n",
    "In case you run this in Google Colab, you will have to install the packages below **(remove the hashtag in front of them)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install damagescanner==0.9b13\n",
    "#!pip install contextily\n",
    "#!pip install exactextract\n",
    "#!pip install lonboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will import all the required Python libraries for data manipulation, spatial analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import shapely\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as cx\n",
    "\n",
    "import damagescanner.download as download\n",
    "from damagescanner.core import DamageScanner\n",
    "from damagescanner.osm import read_osm_data\n",
    "from damagescanner.base_values import DICT_CIS_VULNERABILITY_FLOOD\n",
    "from statistics import mode\n",
    "\n",
    "from lonboard import viz\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning) # exactextract gives a warning that is invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the country of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, we should specify the country for which we want to assess the damage. We use the ISO3 code for the country to download the OpenStreetMap data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_full_name = 'Serbia'\n",
    "country_iso3 = 'SRB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Data\n",
    "In this step, we will load four key datasets:\n",
    "\n",
    "1. **Infrastructure data:**  \n",
    "   This dataset contains information on the location and type of transportation infrastructure (e.g., roads). Each asset may have attributes such as type, length, and replacement cost.\n",
    "\n",
    "2. **Hazard data:**  \n",
    "   This dataset includes information on the hazard affecting the infrastructure (e.g., flood depth at various locations).\n",
    "\n",
    "3. **Vulnerability curves:**  \n",
    "   Vulnerability curves define how the infrastructure's damage increases with the intensity of the hazard. For example, flood depth-damage curves specify how much damage occurs for different flood depths.\n",
    "\n",
    "4. **Maximum damage values:**  \n",
    "   This dataset contains the maximum possible damage (in monetary terms) that can be incurred by individual infrastructure assets. This is crucial for calculating total damage based on the vulnerability curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure Data\n",
    "\n",
    "We will perform this example analysis for Tajikistan. To start the analysis, we first download the OpenStreetMap data from GeoFabrik. In case of locally available data, one can load the shapefiles through **gpd.read_file()**, as we show in the second cell within this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "infrastructure_path = download.get_country_geofabrik(country_iso3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data and read only the road data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 26.7 s\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = read_osm_data(infrastructure_path,asset_type='rail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.9621984, 42.2235539, 22.8397858, 46.1922594])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_types = features.object_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(\"https://github.com/nvkelso/natural-earth-vector/raw/master/10m_cultural/ne_10m_admin_0_countries.shp\")\n",
    "world_plot = world.to_crs(3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin1 = gpd.read_file(\"https://github.com/nvkelso/natural-earth-vector/raw/master/10m_cultural/ne_10m_admin_1_states_provinces.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hazard Data\n",
    "Dealing with extreme heat is generally quite complicated. As extreme temperatures vary greatly across the world, infrastructure is also built differently for that wide range of possible temperatures. Moreover, when it comes to extreme heat, we are most interested on how climate change may change those temperature extremes. However, high-resolution climate data is very data-intensive, and it can be complicated to decide which climate model one should use for the specific region of interest. The latest set of climate projections, clustered within [CMIP-6], contains a large amount of different climate runs and climate models. All combined, they show a large possible range of futures. \n",
    "\n",
    "As such, we strongly suggest to look for regional or nationally-downscaled heat information to perform such an analysis. For the sake of showcasing how one could potentially assess the exposure of extreme heat on infrastructure, we show an example approach using *high resolution climate change observations and projections for the evaluation of heat-related extremes* developed by [**Williams et al., 2024](https://www.nature.com/articles/s41597-024-03074-w). The full dataset can be found [here](https://data.chc.ucsb.edu/products/CHC_CMIP6/). \n",
    "\n",
    "**Please note:** we require quite a lot of data-specific steps to get this analysis working. As such, different to the other hazards, this approach might be more difficult to quickly re-do with other extreme heat data.\n",
    "\n",
    "Here we will show how to extract all data to identify how often temperature exceeded 30 degrees, and how this may change towards the future. We will exemplify this approach for **Serbia** and, as such, only grab the relevant warmest months for **Serbia** (May to September). For different countries, one may want to select different months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first first select the years and months we want to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(x) for x in np.arange(1983,2017,1)]\n",
    "months = ['0'+str(x) for x in np.arange(7,9,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And specify whether we want to look at 30 degrees (`30C`) or 40.6 (`40p6`) degrees plus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tmax_type = '30C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07: 100%|██████████████████████████████████████████████████████████████████████████████| 34/34 [00:20<00:00,  1.62it/s]\n",
      "08: 100%|██████████████████████████████████████████████████████████████████████████████| 34/34 [00:19<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "collect_country_data = {}\n",
    "for month in months:\n",
    "    for year in tqdm(years,total=len(years),desc=month):\n",
    "        file_name = f'https://data.chc.ucsb.edu/products/CHC_CMIP6/extremes/Tmax/observations/{month}/Daily_Tmax_{year}_{month}_cnt_Tmaxgt{Tmax_type}.tif'\n",
    "        hazard_map = xr.open_dataset(file_name, engine=\"rasterio\")\n",
    "\n",
    "        collect_country_data[(month,year)]= hazard_map.rio.clip_box(minx=features.total_bounds[0],\n",
    "                             miny=features.total_bounds[1],\n",
    "                             maxx=features.total_bounds[2],\n",
    "                             maxy=features.total_bounds[3]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07: 100%|██████████████████████████████████████████████████████████████████████████████| 34/34 [12:39<00:00, 22.34s/it]\n",
      "08:  71%|███████████████████████████████████████████████████████                       | 24/34 [09:14<03:48, 22.81s/it]"
     ]
    }
   ],
   "source": [
    "collect_exposure = []\n",
    "for month in months:\n",
    "    for year in tqdm(years,total=len(years),desc=month):\n",
    "        country_hazard_data = collect_country_data[(month,year)]\n",
    "        exposed_features = DamageScanner(country_hazard_data, features, pd.DataFrame(), pd.DataFrame()).exposure(disable_progress=True)#'values']\n",
    "        exposed_features['max_exposure'] = exposed_features.apply(\n",
    "            lambda feature: np.max([x for x in feature['values']]), axis=1\n",
    "        )\n",
    "        exposed_features = exposed_features.rename(columns={'max_exposure': (month,year)})\n",
    "        collect_exposure.append(exposed_features[(month,year)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(collect_exposure,axis=1).groupby(level=1,axis=1).sum().mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_country_data_2050_SSP585 = {}\n",
    "for month in months:\n",
    "    for year in tqdm(years,total=len(years),desc=month):\n",
    "        file_name = f'https://data.chc.ucsb.edu/products/CHC_CMIP6/extremes/Tmax/2050_SSP585/{month}/Daily_Tmax_{year}_{month}_cnt_Tmaxgt{Tmax_type}.tif'\n",
    "        hazard_map = xr.open_dataset(file_name, engine=\"rasterio\")\n",
    "\n",
    "        collect_country_data_2050_SSP585[(month,year)]= hazard_map.rio.clip_box(minx=features.total_bounds[0],\n",
    "                             miny=features.total_bounds[1],\n",
    "                             maxx=features.total_bounds[2],\n",
    "                             maxy=features.total_bounds[3]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_country_data_2050_SSP585[(month,year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_exposure_2050_SSP585 = []\n",
    "for month in months:\n",
    "    for year in tqdm(years,total=len(years),desc=month):\n",
    "        country_hazard_data = collect_country_data_2050_SSP585[(month,year)]\n",
    "        exposed_features = DamageScanner(country_hazard_data, features, pd.DataFrame(), pd.DataFrame()).exposure(disable_progress=True)#'values']\n",
    "        exposed_features['max_exposure'] = exposed_features.apply(\n",
    "            lambda feature: np.max([x for x in feature['values']]), axis=1\n",
    "        )\n",
    "        exposed_features = exposed_features.rename(columns={'max_exposure': (month,year)})\n",
    "        collect_exposure_2050_SSP585.append(exposed_features[(month,year)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(collect_exposure_2050_SSP585,axis=1).groupby(level=1,axis=1).sum().mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
